{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph Multi-Agent Sandbox\n",
    "\n",
    "This notebook provides a testing environment for the Simplify Agent, which uses LangGraph to create a multi-step workflow for simplifying complex topics.\n",
    "\n",
    "**Architecture Overview:**\n",
    "- **State Management**: Defines the graph state and structured output schema\n",
    "- **Model Configuration**: Sets up the LLM with structured output binding\n",
    "- **Agent Graph**: Builds the LangGraph workflow with nodes and edges\n",
    "- **Execution**: Runs the agent with test topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration & Environment Setup\n",
    "\n",
    "Load environment variables and configure the OpenAI API key and model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "LLM_MODEL = \"gpt-4o-mini\"  # Using a valid model\n",
    "\n",
    "# Verify API key is set\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "print(f\"\u2713 Configuration loaded\")\n",
    "print(f\"\u2713 Model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Management & Schema Definition\n",
    "\n",
    "Define the Pydantic schema for structured output and the graph state that flows through the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Structured Output Schema ---\n",
    "class TopicSummary(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for the final, structured output of the agent.\n",
    "    This ensures the LLM's response is easily parsable and high-quality.\n",
    "    \"\"\"\n",
    "    topic: str = Field(description=\"The original topic provided by the user.\")\n",
    "    simplified_text: str = Field(\n",
    "        description=\"A clear, simple explanation of the topic, suitable for a 10-year-old.\"\n",
    "    )\n",
    "    analogy: str = Field(\n",
    "        description=\"A simple, relatable analogy to explain the core concept.\"\n",
    "    )\n",
    "    example: str = Field(\n",
    "        description=\"A real-world or fictional example that illustrates the topic.\"\n",
    "    )\n",
    "\n",
    "# --- Graph State ---\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph's execution.\n",
    "    Data flow across nodes is managed by updating this dictionary.\n",
    "    \"\"\"\n",
    "    # The complex topic provided by the user\n",
    "    user_topic: str\n",
    "    # The final, structured output from the LLM\n",
    "    final_summary: Optional[TopicSummary]\n",
    "\n",
    "print(\"\u2713 State classes defined\")\n",
    "print(f\"\u2713 TopicSummary fields: {list(TopicSummary.model_fields.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Prompt Configuration\n",
    "\n",
    "Initialize the LLM with structured output binding and define the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def get_llm_with_schema():\n",
    "    \"\"\"\n",
    "    Initializes the LLM and binds it to the TopicSummary Pydantic schema\n",
    "    to ensure the output is structured JSON.\n",
    "    \"\"\"\n",
    "    # Initialize the LLM client\n",
    "    llm = ChatOpenAI(\n",
    "        model=LLM_MODEL,\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        temperature=0.2  # Slight temperature for creative parts like analogy/example\n",
    "    )\n",
    "\n",
    "    # Bind the Pydantic schema to the LLM\n",
    "    # This instructs the model to generate a JSON object matching the schema\n",
    "    llm_with_schema = llm.with_structured_output(TopicSummary)\n",
    "    return llm_with_schema\n",
    "\n",
    "def get_summary_prompt():\n",
    "    \"\"\"\n",
    "    Defines the system prompt to guide the LLM's persona and task.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are an expert science communicator and educator. Your goal is to \"\n",
    "        \"take a complex topic and explain it in a clear, highly structured, \"\n",
    "        \"and engaging manner. You must output a JSON object that strictly \"\n",
    "        \"adheres to the provided schema. Do not output any text other than the JSON.\"\n",
    "    )\n",
    "    \n",
    "    # Placeholder for the user's input topic\n",
    "    human_template = \"Topic to explain: {user_topic}\"\n",
    "\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", human_template)\n",
    "    ])\n",
    "\n",
    "# Initialize\n",
    "llm_with_schema = get_llm_with_schema()\n",
    "summary_prompt = get_summary_prompt()\n",
    "\n",
    "print(\"\u2713 LLM initialized with structured output\")\n",
    "print(\"\u2713 Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Graph Nodes & Edge Logic\n",
    "\n",
    "Define the nodes (processing steps) and edge logic (routing decisions) for the LangGraph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "# --- Graph Nodes ---\n",
    "\n",
    "def generate_summary_node(state: GraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Calls the LLM with the structured output schema to generate the summary.\n",
    "    This node attempts to produce the final, structured output.\n",
    "    \"\"\"\n",
    "    print(f\"--- Running Node: Generate Summary for '{state['user_topic']}' ---\")\n",
    "    \n",
    "    # The state is used as the input variables for the prompt\n",
    "    chain = summary_prompt | llm_with_schema\n",
    "    \n",
    "    try:\n",
    "        # Invoke the chain, which forces structured JSON output\n",
    "        result: TopicSummary = chain.invoke(state)\n",
    "        print(\"Summary generated successfully.\")\n",
    "        \n",
    "        # Update the state with the final structured result\n",
    "        return {\"final_summary\": result}\n",
    "        \n",
    "    except OutputParserException as e:\n",
    "        # If the LLM fails to produce valid JSON, handle the error\n",
    "        print(f\"Error: OutputParserException encountered: {e}\")\n",
    "        # Optionally, you could add logic here to re-prompt the LLM\n",
    "        # For this simple example, we will stop and report the error.\n",
    "        return {\"final_summary\": None}\n",
    "\n",
    "\n",
    "# --- Graph Edges (Router) ---\n",
    "\n",
    "def decide_to_end(state: GraphState) -> str:\n",
    "    \"\"\"\n",
    "    A router function that determines the next step.\n",
    "    Since this is a simple, single-step agent, it always proceeds to END.\n",
    "    \"\"\"\n",
    "    print(\"--- Running Router: Decide to End ---\")\n",
    "    \n",
    "    if state.get(\"final_summary\"):\n",
    "        # If we have a summary, we are done\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # If the summary failed to generate (e.g., due to parsing error), we also stop\n",
    "        return \"end\"\n",
    "\n",
    "print(\"\u2713 Graph nodes and edge logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the LangGraph Workflow\n",
    "\n",
    "Assemble all components into a compiled LangGraph application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def build_agent_graph():\n",
    "    \"\"\"\n",
    "    Assembles the LangGraph workflow. \n",
    "    \"\"\"\n",
    "    print(\"--- Building LangGraph Agent ---\")\n",
    "    \n",
    "    # 1. Define the graph state\n",
    "    workflow = StateGraph(GraphState)\n",
    "\n",
    "    # 2. Add nodes\n",
    "    # The main node is the only step that generates content\n",
    "    workflow.add_node(\"summary_generator\", generate_summary_node)\n",
    "\n",
    "    # 3. Set the entry point (first node to run)\n",
    "    workflow.set_entry_point(\"summary_generator\")\n",
    "\n",
    "    # 4. Add the edge to the router\n",
    "    # After the summary is generated, we call the router to decide the next step\n",
    "    workflow.add_conditional_edges(\n",
    "        \"summary_generator\",\n",
    "        decide_to_end,\n",
    "        {\n",
    "            \"end\": END,  # If the router returns \"end\", stop the graph\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 5. Compile the graph into a runnable application\n",
    "    app = workflow.compile()\n",
    "    print(\"--- LangGraph Agent Compiled ---\")\n",
    "    return app\n",
    "\n",
    "# Compile the final agent application\n",
    "agent_app = build_agent_graph()\n",
    "\n",
    "print(\"\u2713 Agent graph compiled and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Graph (Optional)\n",
    "\n",
    "Display the graph structure using Mermaid or ASCII representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to visualize the graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(agent_app.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph visualization: {e}\")\n",
    "    print(\"\\nGraph structure (text):\")\n",
    "    print(agent_app.get_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Execution - Simple Topic\n",
    "\n",
    "Run the agent with a simple test topic to verify the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(topic: str):\n",
    "    \"\"\"\n",
    "    Runs the compiled LangGraph agent with a specific topic.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Agent for Topic: '{topic}' ---\")\n",
    "\n",
    "    # The initial state contains only the user's topic\n",
    "    initial_state: GraphState = {\"user_topic\": topic, \"final_summary\": None}\n",
    "\n",
    "    # Invoke the agent\n",
    "    final_state: GraphState = agent_app.invoke(initial_state)\n",
    "\n",
    "    # --- Print Final Output ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- FINAL RESULT ---\")\n",
    "    \n",
    "    summary: Optional[TopicSummary] = final_state.get(\"final_summary\")\n",
    "    \n",
    "    if summary:\n",
    "        print(f\"Topic: {summary.topic}\")\n",
    "        print(\"\\n--- Simplified Text ---\")\n",
    "        print(summary.simplified_text)\n",
    "        print(\"\\n--- Analogy ---\")\n",
    "        print(summary.analogy)\n",
    "        print(\"\\n--- Example ---\")\n",
    "        print(summary.example)\n",
    "    else:\n",
    "        print(\"Agent failed to produce a structured summary. Check logs for errors.\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Test with a simple topic\n",
    "test_topic_1 = \"Photosynthesis\"\n",
    "result_1 = run_agent(test_topic_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Human-in-the-Loop Feedback System\n",
    "\n",
    "This section implements an interactive feedback loop where you can:\n",
    "- Review generated explanations\n",
    "- Approve with \"yes\" or request regeneration with \"regenerate\"\n",
    "- Track conversation history across attempts\n",
    "- See improvements with each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Enhanced State with Conversation History\n",
    "\n",
    "Extend the graph state to track feedback and previous attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional, List, Dict, Any\n",
    "\n",
    "# Enhanced state for feedback loop\n",
    "class FeedbackGraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Extended state that includes conversation history and feedback.\n",
    "    \"\"\"\n",
    "    # Original fields\n",
    "    user_topic: str\n",
    "    final_summary: Optional[TopicSummary]\n",
    "    \n",
    "    # New fields for feedback loop\n",
    "    conversation_history: List[Dict[str, Any]]  # List of previous attempts\n",
    "    feedback: Optional[str]  # \"yes\" or \"regenerate\"\n",
    "    attempt_count: int  # Current attempt number\n",
    "    max_attempts: int  # Maximum regeneration attempts\n",
    "    user_notes: Optional[str]  # Optional feedback on what to improve\n",
    "\n",
    "print(\"\u2713 FeedbackGraphState defined\")\n",
    "print(\"\u2713 New fields: conversation_history, feedback, attempt_count, max_attempts, user_notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Graph Nodes with Feedback Logic\n",
    "\n",
    "Define nodes that handle generation with context and human feedback collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_with_history_node(state: FeedbackGraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a summary with context from previous attempts.\n",
    "    If this is a regeneration, uses higher temperature and explicit instructions.\n",
    "    \"\"\"\n",
    "    attempt_num = state['attempt_count'] + 1\n",
    "    print(f\"\\n--- Attempt #{attempt_num}: Generating Summary for '{state['user_topic']}' ---\")\n",
    "    \n",
    "    # Build the prompt based on whether this is a regeneration\n",
    "    if state['conversation_history']:\n",
    "        # This is a REGENERATION - use higher temperature and detailed context\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        from langchain_core.prompts import ChatPromptTemplate\n",
    "        \n",
    "        # Create LLM with HIGHER temperature for more variation\n",
    "        llm_regenerate = ChatOpenAI(\n",
    "            model=LLM_MODEL,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            temperature=0.9  # Much higher for creative regeneration!\n",
    "        )\n",
    "        llm_regenerate_with_schema = llm_regenerate.with_structured_output(TopicSummary)\n",
    "        \n",
    "        # Build detailed context from previous attempts\n",
    "        previous_attempts = state['conversation_history']\n",
    "        context_parts = []\n",
    "        \n",
    "        context_parts.append(f\"\\n\\nPREVIOUS ATTEMPTS ({len(previous_attempts)}):\")\n",
    "        context_parts.append(\"The user rejected these explanations. You MUST create something COMPLETELY DIFFERENT.\\n\")\n",
    "        \n",
    "        for i, attempt in enumerate(previous_attempts, 1):\n",
    "            s = attempt['summary']\n",
    "            context_parts.append(f\"\\n--- Rejected Attempt {i} ---\")\n",
    "            context_parts.append(f\"Simplified: {s.simplified_text}\")\n",
    "            context_parts.append(f\"Analogy Used: {s.analogy}\")\n",
    "            context_parts.append(f\"Example Used: {s.example}\")\n",
    "            if attempt.get('user_notes'):\n",
    "                context_parts.append(f\"User Feedback: {attempt['user_notes']}\")\n",
    "        \n",
    "        context_parts.append(\"\\n\\n=== YOUR TASK ===\")\n",
    "        context_parts.append(\"Generate a COMPLETELY NEW explanation that:\")\n",
    "        context_parts.append(\"1. Uses a DIFFERENT analogy (not similar to any above)\")\n",
    "        context_parts.append(\"2. Uses a DIFFERENT example (from a different domain)\")\n",
    "        context_parts.append(\"3. Explains the concept from a DIFFERENT angle\")\n",
    "        context_parts.append(\"4. Uses DIFFERENT vocabulary and phrasing\")\n",
    "        context_parts.append(\"5. Is MORE creative and engaging than previous attempts\")\n",
    "        \n",
    "        context_info = '\\n'.join(context_parts)\n",
    "        \n",
    "        # Enhanced prompt for regeneration\n",
    "        enhanced_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", (\n",
    "                \"You are a creative science communicator. The user rejected your previous explanations. \"\n",
    "                \"You MUST generate something COMPLETELY DIFFERENT with fresh perspectives. \"\n",
    "                \"Use your creativity to find NEW analogies, NEW examples, and NEW ways to explain. \"\n",
    "                \"Do NOT repeat or slightly modify previous attempts. Be bold and innovative! \"\n",
    "                \"Output only valid JSON matching the schema.\"\n",
    "            )),\n",
    "            (\"human\", \"Topic: {user_topic}\" + context_info)\n",
    "        ])\n",
    "        \n",
    "        chain = enhanced_prompt | llm_regenerate_with_schema\n",
    "        print(f\"\ud83d\udd25 Using HIGH temperature (0.9) for creative regeneration\")\n",
    "    else:\n",
    "        # First attempt - use standard prompt\n",
    "        chain = summary_prompt | llm_with_schema\n",
    "        print(f\"\ud83d\udcdd Using standard temperature (0.2) for first attempt\")\n",
    "    \n",
    "    try:\n",
    "        # Generate the summary\n",
    "        result: TopicSummary = chain.invoke(state)\n",
    "        print(f\"\u2713 Summary generated successfully (Attempt #{attempt_num})\")\n",
    "        \n",
    "        # Add to conversation history\n",
    "        new_history = state['conversation_history'].copy()\n",
    "        new_history.append({\n",
    "            'attempt': attempt_num,\n",
    "            'summary': result,\n",
    "            'feedback': None  # Will be filled in by human feedback node\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"final_summary\": result,\n",
    "            \"conversation_history\": new_history,\n",
    "            \"attempt_count\": attempt_num\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error generating summary: {e}\")\n",
    "        return {\"final_summary\": None}\n",
    "\n",
    "\n",
    "def human_feedback_node(state: FeedbackGraphState) -> dict:\n",
    "    \"\"\"\n",
    "    Collects human feedback on the generated summary.\n",
    "    Asks the user to approve (yes) or request regeneration (regenerate).\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\ud83d\udccb GENERATED EXPLANATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    summary = state['final_summary']\n",
    "    if summary:\n",
    "        print(f\"\\n\ud83c\udfaf Topic: {summary.topic}\")\n",
    "        print(f\"\\n\ud83d\udcdd Simplified Explanation:\\n{summary.simplified_text}\")\n",
    "        print(f\"\\n\ud83d\udd04 Analogy:\\n{summary.analogy}\")\n",
    "        print(f\"\\n\ud83d\udca1 Example:\\n{summary.example}\")\n",
    "    else:\n",
    "        print(\"\\n\u274c No summary was generated.\")\n",
    "        return {\"feedback\": \"end\"}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Attempt {state['attempt_count']} of {state['max_attempts']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get user feedback\n",
    "    while True:\n",
    "        user_input = input(\"\\n\ud83d\udc49 Enter 'yes' to approve or 'regenerate' to try again: \").strip().lower()\n",
    "        \n",
    "        if user_input in ['yes', 'y']:\n",
    "            print(\"\\n\u2705 Explanation approved!\")\n",
    "            # Update the last history entry with feedback\n",
    "            updated_history = state['conversation_history'].copy()\n",
    "            if updated_history:\n",
    "                updated_history[-1]['feedback'] = 'approved'\n",
    "            return {\n",
    "                \"feedback\": \"yes\",\n",
    "                \"conversation_history\": updated_history\n",
    "            }\n",
    "        \n",
    "        elif user_input in ['regenerate', 'r', 'no', 'n']:\n",
    "            # Optional: ask what to improve\n",
    "            notes = input(\"\\n\ud83d\udcad (Optional) What should be improved? Press Enter to skip: \").strip()\n",
    "            \n",
    "            print(\"\\n\ud83d\udd04 Regenerating with improvements...\")\n",
    "            # Update the last history entry with feedback\n",
    "            updated_history = state['conversation_history'].copy()\n",
    "            if updated_history:\n",
    "                updated_history[-1]['feedback'] = 'regenerate'\n",
    "                if notes:\n",
    "                    updated_history[-1]['user_notes'] = notes\n",
    "            \n",
    "            return {\n",
    "                \"feedback\": \"regenerate\",\n",
    "                \"user_notes\": notes if notes else None,\n",
    "                \"conversation_history\": updated_history\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            print(\"\u274c Invalid input. Please enter 'yes' or 'regenerate'.\")\n",
    "\n",
    "print(\"\u2713 Feedback nodes defined (with HIGH temperature regeneration!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Routing Logic\n",
    "\n",
    "Define how the graph routes based on user feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_feedback(state: FeedbackGraphState) -> str:\n",
    "    \"\"\"\n",
    "    Routes the graph based on user feedback.\n",
    "    - 'yes' \u2192 END (user approved)\n",
    "    - 'regenerate' \u2192 generate_summary (try again)\n",
    "    - Check max attempts to prevent infinite loops\n",
    "    \"\"\"\n",
    "    feedback = state.get('feedback')\n",
    "    attempt_count = state.get('attempt_count', 0)\n",
    "    max_attempts = state.get('max_attempts', 3)\n",
    "    \n",
    "    print(f\"\\n--- Router: Processing feedback '{feedback}' (Attempt {attempt_count}/{max_attempts}) ---\")\n",
    "    \n",
    "    # If user approved, end the workflow\n",
    "    if feedback == 'yes':\n",
    "        print(\"\u2192 Routing to END (approved)\")\n",
    "        return \"end\"\n",
    "    \n",
    "    # If user wants to regenerate\n",
    "    elif feedback == 'regenerate':\n",
    "        # Check if we've hit max attempts\n",
    "        if attempt_count >= max_attempts:\n",
    "            print(f\"\u26a0\ufe0f  Maximum attempts ({max_attempts}) reached. Ending workflow.\")\n",
    "            return \"end\"\n",
    "        else:\n",
    "            print(\"\u2192 Routing back to GENERATE (regenerate)\")\n",
    "            return \"regenerate\"\n",
    "    \n",
    "    # Default: end\n",
    "    else:\n",
    "        print(\"\u2192 Routing to END (default)\")\n",
    "        return \"end\"\n",
    "\n",
    "print(\"\u2713 Routing logic defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Build the Feedback Loop Graph\n",
    "\n",
    "Assemble the graph with feedback loop capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def build_feedback_agent_graph():\n",
    "    \"\"\"\n",
    "    Builds a LangGraph workflow with human-in-the-loop feedback.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Feedback Loop Agent ---\")\n",
    "    \n",
    "    # 1. Create the graph with FeedbackGraphState\n",
    "    workflow = StateGraph(FeedbackGraphState)\n",
    "    \n",
    "    # 2. Add nodes\n",
    "    workflow.add_node(\"generate_summary\", generate_summary_with_history_node)\n",
    "    workflow.add_node(\"human_feedback\", human_feedback_node)\n",
    "    \n",
    "    # 3. Set entry point\n",
    "    workflow.set_entry_point(\"generate_summary\")\n",
    "    \n",
    "    # 4. Add edges\n",
    "    # After generation, always go to human feedback\n",
    "    workflow.add_edge(\"generate_summary\", \"human_feedback\")\n",
    "    \n",
    "    # After feedback, route based on user input\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_feedback\",\n",
    "        route_after_feedback,\n",
    "        {\n",
    "            \"end\": END,\n",
    "            \"regenerate\": \"generate_summary\"  # Loop back!\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 5. Compile\n",
    "    app = workflow.compile()\n",
    "    print(\"\u2713 Feedback Loop Agent compiled\")\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Build the feedback agent\n",
    "feedback_agent = build_feedback_agent_graph()\n",
    "\n",
    "print(\"\\n\u2705 Feedback agent ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Visualize the Feedback Loop\n",
    "\n",
    "See the graph structure with the feedback loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the feedback graph\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(feedback_agent.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(f\"Could not display graph: {e}\")\n",
    "    print(\"\\nGraph structure:\")\n",
    "    print(feedback_agent.get_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Run the Feedback Loop Agent\n",
    "\n",
    "Test the agent with human-in-the-loop feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_with_feedback(topic: str, max_attempts: int = 3):\n",
    "    \"\"\"\n",
    "    Runs the feedback loop agent with a given topic.\n",
    "    \n",
    "    Args:\n",
    "        topic: The complex topic to simplify\n",
    "        max_attempts: Maximum number of regeneration attempts (default: 3)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"\ud83d\ude80 STARTING FEEDBACK LOOP AGENT\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Max Attempts: {max_attempts}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Initialize state\n",
    "    initial_state: FeedbackGraphState = {\n",
    "        \"user_topic\": topic,\n",
    "        \"final_summary\": None,\n",
    "        \"conversation_history\": [],\n",
    "        \"feedback\": None,\n",
    "        \"attempt_count\": 0,\n",
    "        \"max_attempts\": max_attempts,\n",
    "        \"user_notes\": None\n",
    "    }\n",
    "    \n",
    "    # Run the agent\n",
    "    final_state: FeedbackGraphState = feedback_agent.invoke(initial_state)\n",
    "    \n",
    "    # Display final results\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"\ud83d\udcca FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\n\u2713 Total Attempts: {final_state['attempt_count']}\")\n",
    "    print(f\"\u2713 Final Decision: {final_state.get('feedback', 'N/A')}\")\n",
    "    \n",
    "    # Show conversation history\n",
    "    if final_state['conversation_history']:\n",
    "        print(f\"\\n\ud83d\udcdc CONVERSATION HISTORY:\\n\")\n",
    "        for entry in final_state['conversation_history']:\n",
    "            print(f\"  Attempt {entry['attempt']}:\")\n",
    "            print(f\"    Feedback: {entry.get('feedback', 'N/A')}\")\n",
    "            if entry.get('user_notes'):\n",
    "                print(f\"    Notes: {entry['user_notes']}\")\n",
    "            print()\n",
    "    \n",
    "    # Show final approved summary\n",
    "    if final_state.get('final_summary'):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"\u2705 APPROVED EXPLANATION\")\n",
    "        print(\"=\"*70)\n",
    "        summary = final_state['final_summary']\n",
    "        print(f\"\\n\ud83c\udfaf Topic: {summary.topic}\")\n",
    "        print(f\"\\n\ud83d\udcdd Simplified:\\n{summary.simplified_text}\")\n",
    "        print(f\"\\n\ud83d\udd04 Analogy:\\n{summary.analogy}\")\n",
    "        print(f\"\\n\ud83d\udca1 Example:\\n{summary.example}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "print(\"\u2713 Interactive testing function ready\")\n",
    "print(\"\\n\ud83d\udca1 Usage: run_agent_with_feedback('Your Topic Here', max_attempts=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Example: Test with a Topic\n",
    "\n",
    "Try the feedback loop! You'll be prompted to approve or regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the feedback loop\n",
    "result = run_agent_with_feedback(\"Quantum Entanglement\", max_attempts=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}